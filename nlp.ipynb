{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/iga_niemiec/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/iga_niemiec/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import datetime\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_path = \"./analysts.json\" \n",
    "target_words = [\"Tesla\", \"Musk\"]\n",
    "start_datetime = datetime.datetime(2019, 10, 19, 11, 20, 0, 0)\n",
    "# TODO target words from a family of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and punctation\n",
    "\n",
    "def remove_stop_and_punctation (input_text):\n",
    "    \n",
    "    input_text = input_text.lower()\n",
    "    \n",
    "    p = re.compile(r'\\w+')\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "    word_tokens = word_tokenize(input_text) \n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words and bool(p.match(w)): \n",
    "            filtered_sentence.append(w) \n",
    "  \n",
    "    return(filtered_sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# TODO decide if its better or worse with that\n",
    "\n",
    "def stem_words (input_text):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_sentence = []\n",
    "\n",
    "    for word in input_text:\n",
    "        stemmed_sentence.append(ps.stem(word))\n",
    "    \n",
    "    return(stemmed_sentence)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating nr of words in string\n",
    "def calculate_words_nr (stemmed_text, target_words):\n",
    "    \n",
    "    target_words = [word.lower() for word in target_words]\n",
    "    stemmed_words = stem_words(target_words)\n",
    "    \n",
    "    words_nr = sum(word in stemmed_words for word in stemmed_text)\n",
    "    \n",
    "    return(words_nr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datetime from timestamps\n",
    "def convert_stamps_to_datetime (start_datetime, stamp):\n",
    "    # stamps in milisec\n",
    "    stamp = 8196\n",
    "    stamp_us = stamp * 1000\n",
    "\n",
    "    tst_datetime = start_datetime + datetime.timedelta(0,0, stamp_us)\n",
    "    tst_datetime = tst_datetime.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return(tst_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this highly unusual and controversial earnings call for Tesla is at the top of Wall Street so you can see\n",
      "['Tesla', 'Musk']\n",
      "['highly', 'unusual', 'controversial', 'earnings', 'call', 'tesla', 'top', 'wall', 'street', 'see']\n",
      "['highli', 'unusu', 'controversi', 'earn', 'call', 'tesla', 'top', 'wall', 'street', 'see']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "input_text_no_stop = remove_stop_and_punctation(input_text)\n",
    "input_text_stemmed = stem_words(input_text_no_stop)\n",
    "words_nr = calculate_words_nr(input_text_stemmed, target_words)\n",
    "\n",
    "print(input_text)\n",
    "print(target_words)\n",
    "print(input_text_no_stop)\n",
    "print(input_text_stemmed)\n",
    "print(words_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json file and return jsonwith word counting info\n",
    "\n",
    "def count_words_in_json (json_path, target_words, start_datetime):\n",
    "\n",
    "    with open(json_path) as fhandler:\n",
    "    \n",
    "        parsed_json = json.load(fhandler)\n",
    "        \n",
    "    for obj in parsed_json:\n",
    "        text = obj['text']\n",
    "        text = remove_stop_and_punctation(text)\n",
    "        text = stem_words(text)\n",
    "        words_nr = calculate_words_nr(text, target_words)\n",
    "        stamp_datetime = convert_stamps_to_datetime(start_datetime, obj['time'])\n",
    "    \n",
    "        obj['text'] = obj['text']\n",
    "        obj['text_clean'] = text\n",
    "        obj['time'] = stamp_datetime\n",
    "        obj['words_nr'] = words_nr\n",
    "    \n",
    "    with open(\"./analyst_counted.json\", \"w\") as fhandler:\n",
    "    \n",
    "        json.dump(parsed_json, fhandler)\n",
    "        \n",
    "    return(parsed_json)\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['highli', 'unusu', 'controversi', 'earn', 'call', 'tesla', 'top', 'wall', 'street', 'see']\n",
      "1\n",
      "2019-10-19 11:20:08.196000\n",
      "['stock', 'sink']\n",
      "0\n",
      "2019-10-19 11:20:08.196000\n",
      "['minut', 'ceo', 'elon', 'musk', 'start', 'insult', 'analyst']\n",
      "1\n",
      "2019-10-19 11:20:08.196000\n",
      "['insult', 'guess']\n",
      "0\n",
      "2019-10-19 11:20:08.196000\n",
      "['pull', 'controversi', 'moment', 'call', 'listen', 'judg']\n",
      "0\n",
      "2019-10-19 11:20:08.196000\n",
      "[{'text': 'this highly unusual and controversial earnings call for Tesla is at the top of Wall Street so you can see', 'time': '2019-10-19 11:20:08.196000', 'text_clean': ['highli', 'unusu', 'controversi', 'earn', 'call', 'tesla', 'top', 'wall', 'street', 'see'], 'words_nr': 1}, {'text': 'a stock sinking', 'time': '2019-10-19 11:20:08.196000', 'text_clean': ['stock', 'sink'], 'words_nr': 0}, {'text': 'the minute CEO Elon Musk started insulting analysts', 'time': '2019-10-19 11:20:08.196000', 'text_clean': ['minut', 'ceo', 'elon', 'musk', 'start', 'insult', 'analyst'], 'words_nr': 1}, {'text': \"if I'm insulting his own guess what we did was\", 'time': '2019-10-19 11:20:08.196000', 'text_clean': ['insult', 'guess'], 'words_nr': 0}, {'text': 'we pulled up some of the most controversial moments during the call listen and judge for yourself', 'time': '2019-10-19 11:20:08.196000', 'text_clean': ['pull', 'controversi', 'moment', 'call', 'listen', 'judg'], 'words_nr': 0}]\n"
     ]
    }
   ],
   "source": [
    "for obj in parsed_json:\n",
    "    text = obj['text']\n",
    "    text = remove_stop_and_punctation(text)\n",
    "    text = stem_words(text)\n",
    "    words_nr = calculate_words_nr(text, target_words)\n",
    "    stamp_datetime = convert_stamps_to_datetime(start_datetime, obj['time'])\n",
    "    \n",
    "    print(text)\n",
    "    print(words_nr)\n",
    "    print(stamp_datetime)\n",
    "    \n",
    "    obj['text'] = obj['text']\n",
    "    obj['text_clean'] = text\n",
    "    obj['time'] = stamp_datetime\n",
    "    obj['words_nr'] = words_nr\n",
    "    \n",
    "print(parsed_json)\n",
    "\n",
    "\n",
    "with open(\"./analyst_counted.json\", \"w\") as fhandler:\n",
    "    \n",
    "    json.dump(parsed_json, fhandler)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
